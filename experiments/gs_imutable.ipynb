{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1622d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from explainers_lib.explainers.carla.growing_spheres import GrowingSpheresExplainer\n",
    "# from explainers_lib.explainers import AlibiCFRL\n",
    "from explainers_lib.aggregators import Pareto, All\n",
    "from explainers_lib.datasets import Dataset\n",
    "from explainers_lib.ensemble import Ensemble\n",
    "from explainers_lib.model import TorchModel\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ebf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(['Name'], axis=1)\n",
    "categorical_features = ['Sex', 'Pclass']\n",
    "numerical_features = ['Age', 'Fare', 'Parents/Children Aboard', 'Siblings/Spouses Aboard']\n",
    "target = 'Survived'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8e762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class InverseColumnTransformer(ColumnTransformer):\n",
    "    \"\"\"\n",
    "    A custom ColumnTransformer that adds an 'inverse_transform' method.\n",
    "\n",
    "    IMPORTANT: This implementation is tightly coupled to the *specific*\n",
    "    pipelines provided in the user's question. It assumes:\n",
    "    1. A 'num' transformer pipeline with a 'scaler' step.\n",
    "    2. A 'cat' transformer pipeline with an 'onehot' step.\n",
    "    3. 'remainder' is set to 'passthrough'.\n",
    "    \n",
    "    It \"inverts\" the data by reversing only the reversible steps\n",
    "    (scaler, onehot) while ignoring the non-reversible SimpleImputers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Override fit to store original DataFrame columns\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer and stores the original column names\n",
    "        and order if X is a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.original_columns_ = list(X.columns)\n",
    "        else:\n",
    "            # Fallback for numpy arrays\n",
    "            self.original_columns_ = list(range(X.shape[1]))\n",
    "            \n",
    "        # --- BUG FIX 1: This line is ESSENTIAL ---\n",
    "        # It calls the parent's fit method, which fits all the\n",
    "        # transformers (pipelines) and populates self.transformers_\n",
    "        # return super().fit(X, y)\n",
    "\n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Reverses the scaling and one-hot encoding steps to return\n",
    "        a NumPy array as close to the original as possible.\n",
    "        \n",
    "        Imputed values are NOT reversed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- Improved Input Validation ---\n",
    "        # Ensure X_transformed is a numpy array\n",
    "        if not isinstance(X_transformed, np.ndarray):\n",
    "            X_transformed = np.array(X_transformed)\n",
    "\n",
    "        # Handle 1D array (e.g., a single prediction)\n",
    "        if X_transformed.ndim == 1:\n",
    "            X_transformed = X_transformed.reshape(1, -1)\n",
    "            \n",
    "        # Now, ensure it's 2D\n",
    "        if X_transformed.ndim != 2:\n",
    "            raise ValueError(f\"X_transformed must be a 1D or 2D array, but got shape {X_transformed.shape}\")\n",
    "            \n",
    "        if not hasattr(self, 'original_columns_'):\n",
    "            raise ValueError(\"The 'fit' method must be called before 'inverse_transform'.\")\n",
    "\n",
    "        inverted_parts = {}\n",
    "        current_col_idx = 0\n",
    "\n",
    "        # Iterate over the fitted transformers\n",
    "        for name, transformer, original_cols in self.transformers_:\n",
    "            \n",
    "            if transformer == 'drop':\n",
    "                continue # Dropped columns are ignored\n",
    "\n",
    "            elif transformer == 'passthrough':\n",
    "                # This handles the 'remainder'\n",
    "                n_features = len(original_cols)\n",
    "                if n_features == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Slicing the numpy array\n",
    "                part_data = X_transformed[:, current_col_idx : current_col_idx + n_features]\n",
    "                # Storing as a DataFrame for easy combining later\n",
    "                inverted_parts[name] = pd.DataFrame(part_data, columns=original_cols)\n",
    "                current_col_idx += n_features\n",
    "\n",
    "            else:\n",
    "                # This is a Pipeline\n",
    "                if name == 'num':\n",
    "                    # --- Specific to the 'num' pipeline ---\n",
    "                    try:\n",
    "                        scaler = transformer.named_steps['scaler']\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"The 'num' pipeline must have a step named 'scaler'.\")\n",
    "                        \n",
    "                    n_features = len(original_cols)\n",
    "                    part_data = X_transformed[:, current_col_idx : current_col_idx + n_features]\n",
    "                    \n",
    "                    # Invert only the 'scaler' step\n",
    "                    inverted_data = scaler.inverse_transform(part_data)\n",
    "                    inverted_parts[name] = pd.DataFrame(inverted_data, columns=original_cols)\n",
    "                    current_col_idx += n_features\n",
    "\n",
    "                elif name == 'cat':\n",
    "                    # --- Specific to the 'cat' pipeline ---\n",
    "                    try:\n",
    "                        onehot = transformer.named_steps['onehot']\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"The 'cat' pipeline must have a step named 'onehot'.\")\n",
    "                        \n",
    "                    # Get the *output* feature count from the one-hot encoder\n",
    "                    try:\n",
    "                        n_output_features = len(onehot.get_feature_names_out(original_cols))\n",
    "                    except Exception:\n",
    "                        # Fallback if get_feature_names_out fails\n",
    "                        n_output_features = sum(len(c) for c in onehot.categories_)\n",
    "                        \n",
    "                    part_data = X_transformed[:, current_col_idx : current_col_idx + n_output_features]\n",
    "                    \n",
    "                    # Invert only the 'onehot' step\n",
    "                    inverted_data = onehot.inverse_transform(part_data)\n",
    "                    inverted_parts[name] = pd.DataFrame(inverted_data, columns=original_cols)\n",
    "                    current_col_idx += n_output_features\n",
    "                \n",
    "                else:\n",
    "                    # Handle other pipelines\n",
    "                    raise ValueError(f\"Unknown transformer '{name}' is not supported for inversion.\")\n",
    "\n",
    "        # Concatenate all inverted DataFrames\n",
    "        try:\n",
    "            df_inverted = pd.concat(inverted_parts.values(), axis=1)\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Failed to concatenate inverted parts. Check for index mismatches. Error: {e}\")\n",
    "\n",
    "        # Re-order to match the original DataFrame's column order\n",
    "        df_inverted_original_order = df_inverted[self.original_columns_]\n",
    "        \n",
    "        return df_inverted_original_order.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294ea2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = InverseColumnTransformer(transformers=[('num', numerical_transformer, numerical_features),('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb2c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Fare', 'Parents/Children Aboard', 'Siblings/Spouses Aboard', 'Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "8",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2d18d1dc-d90d-4bb8-8971-c9348c56af04",
       "rows": [
        [
         "0",
         "1.254299635485627",
         "-0.0576315785685861",
         "-0.4750089428705203",
         "-0.4703877064610524",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "-0.30706712859510227",
         "0.15190188135429955",
         "1.9922536248598743",
         "0.40266172778210496",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0"
        ],
        [
         "2",
         "2.8156663995663562",
         "-0.3602288201353211",
         "-0.4750089428705203",
         "-0.4703877064610524",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0"
        ],
        [
         "3",
         "1.8930405844277436",
         "0.03693005942101858",
         "-0.4750089428705203",
         "-0.4703877064610524",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "1.9640118009768677",
         "-0.40089032447085104",
         "-0.4750089428705203",
         "-0.4703877064610524",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.254300</td>\n",
       "      <td>-0.057632</td>\n",
       "      <td>-0.475009</td>\n",
       "      <td>-0.470388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.307067</td>\n",
       "      <td>0.151902</td>\n",
       "      <td>1.992254</td>\n",
       "      <td>0.402662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.815666</td>\n",
       "      <td>-0.360229</td>\n",
       "      <td>-0.475009</td>\n",
       "      <td>-0.470388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.893041</td>\n",
       "      <td>0.036930</td>\n",
       "      <td>-0.475009</td>\n",
       "      <td>-0.470388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.964012</td>\n",
       "      <td>-0.400890</td>\n",
       "      <td>-0.475009</td>\n",
       "      <td>-0.470388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3    4    5    6    7    8\n",
       "0  1.254300 -0.057632 -0.475009 -0.470388  0.0  1.0  1.0  0.0  0.0\n",
       "1 -0.307067  0.151902  1.992254  0.402662  0.0  1.0  0.0  1.0  0.0\n",
       "2  2.815666 -0.360229 -0.475009 -0.470388  0.0  1.0  0.0  0.0  1.0\n",
       "3  1.893041  0.036930 -0.475009 -0.470388  0.0  1.0  1.0  0.0  0.0\n",
       "4  1.964012 -0.400890 -0.475009 -0.470388  0.0  1.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "preprocessor.fit(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_test_processed)\n",
    "\n",
    "data = Dataset(X_test_processed, \n",
    "               y_test.values, \n",
    "               feature_names, \n",
    "               list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)), \n",
    "               numerical_features, \n",
    "               [i for i in list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)) if \"Sex\" in i] + [\"Age\"], \n",
    "               [])\n",
    "print(feature_names)\n",
    "pd.DataFrame(X_test_processed).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8ea973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<explainers_lib.model.TorchModel at 0x7f513c92ba10>, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train_processed.shape[1]\n",
    "with open(\"experiments/models/titanic_classifier.pt\", \"rb\") as f:\n",
    "    model_data = f.read()\n",
    "\n",
    "model = TorchModel.deserialize(model_data)\n",
    "model, input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63562d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5instance [00:00,  7.15instance/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Pclass </span>┃<span style=\"font-weight: bold\">  Sex </span>┃<span style=\"font-weight: bold\">     Age </span>┃<span style=\"font-weight: bold\"> Siblings/Spouses Aboard </span>┃<span style=\"font-weight: bold\"> Parents/Children Aboard </span>┃<span style=\"font-weight: bold\">    Fare </span>┃<span style=\"font-weight: bold\"> target </span>┃<span style=\"font-weight: bold\"> source         </span>┃\n",
       "┡━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│ 1.0000 │ male │ 47.0000 │                  0.0000 │                  0.0000 │ 30.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 47.0000 │                  0.6711 │                 -0.6466 │ 38.0561 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 2.0000 │ male │ 25.0000 │                  1.0000 │                  2.0000 │ 41.5792 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 25.0000 │                  0.9385 │                  1.9678 │ 51.5468 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 3.0000 │ male │ 69.0000 │                  0.0000 │                  0.0000 │ 14.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 69.0000 │                  0.3612 │                 -0.1159 │ 25.0907 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 1.0000 │ male │ 56.0000 │                  0.0000 │                  0.0000 │ 35.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 56.0000 │                  0.6082 │                 -0.4301 │ 50.1290 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 2.0000 │ male │ 57.0000 │                  0.0000 │                  0.0000 │ 12.3500 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 57.0000 │                  0.7739 │                 -0.2928 │ 24.5178 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "└────────┴──────┴─────────┴─────────────────────────┴─────────────────────────┴─────────┴────────┴────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPclass\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Sex\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Age\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSiblings/Spouses Aboard\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mParents/Children Aboard\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Fare\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtarget\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msource        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│ 1.0000 │ male │ 47.0000 │                  0.0000 │                  0.0000 │ 30.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 47.0000 │                  0.6711 │                 -0.6466 │ 38.0561 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 2.0000 │ male │ 25.0000 │                  1.0000 │                  2.0000 │ 41.5792 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 25.0000 │                  0.9385 │                  1.9678 │ 51.5468 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 3.0000 │ male │ 69.0000 │                  0.0000 │                  0.0000 │ 14.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 69.0000 │                  0.3612 │                 -0.1159 │ 25.0907 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 1.0000 │ male │ 56.0000 │                  0.0000 │                  0.0000 │ 35.5000 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 56.0000 │                  0.6082 │                 -0.4301 │ 50.1290 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "├────────┼──────┼─────────┼─────────────────────────┼─────────────────────────┼─────────┼────────┼────────────────┤\n",
       "│ 2.0000 │ male │ 57.0000 │                  0.0000 │                  0.0000 │ 12.3500 │      0 │ original data  │\n",
       "│ 1.0000 │ male │ 57.0000 │                  0.7739 │                 -0.2928 │ 24.5178 │      1 │ growing_spher… │\n",
       "│        │      │         │                         │                         │         │        │ max_iter=1000, │\n",
       "│        │      │         │                         │                         │         │        │ num_samples=1… │\n",
       "└────────┴──────┴─────────┴─────────────────────────┴─────────────────────────┴─────────┴────────┴────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble = Ensemble(model,[GrowingSpheresExplainer()], All())\n",
    "\n",
    "ensemble.fit(data)\n",
    "cfs = ensemble.explain(data[:5], pretty_print=True, pretty_print_postprocess=preprocessor.inverse_transform, feature_names=preprocessor.original_columns_)\n",
    "\n",
    "# cfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba794f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counterfactual(original_data=array([ 1.25429964, -0.05763158, -0.47500894, -0.47038771,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ]), data=array([ 1.25429964,  0.08527239, -1.27271644,  0.11548021,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ]), original_class=np.int64(0), target_class=1, explainer='growing_spheres(step_size=0.2, max_iter=1000, num_samples=1000)'),\n",
       " Counterfactual(original_data=array([-0.30706713,  0.15190188,  1.99225362,  0.40266173,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ]), data=array([-0.30706713,  0.34041251,  1.95247168,  0.34898546,  0.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.        ]), original_class=np.int64(0), target_class=1, explainer='growing_spheres(step_size=0.2, max_iter=1000, num_samples=1000)'),\n",
       " Counterfactual(original_data=array([ 2.8156664 , -0.36022882, -0.47500894, -0.47038771,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  1.        ]), data=array([ 2.8156664 , -0.15993313, -0.61795086, -0.15503196,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ]), original_class=np.int64(0), target_class=1, explainer='growing_spheres(step_size=0.2, max_iter=1000, num_samples=1000)'),\n",
       " Counterfactual(original_data=array([ 1.89304058,  0.03693006, -0.47500894, -0.47038771,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ]), data=array([ 1.89304058,  0.3135987 , -1.00559094,  0.06058989,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ]), original_class=np.int64(0), target_class=1, explainer='growing_spheres(step_size=0.2, max_iter=1000, num_samples=1000)'),\n",
       " Counterfactual(original_data=array([ 1.9640118 , -0.40089032, -0.47500894, -0.47038771,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ]), data=array([ 1.9640118 , -0.17076982, -0.8362578 ,  0.20529256,  0.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.        ]), original_class=np.int64(0), target_class=1, explainer='growing_spheres(step_size=0.2, max_iter=1000, num_samples=1000)')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
